{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da941e78",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1857a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e389a",
   "metadata": {},
   "source": [
    "#### Chat model - OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9149c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "#   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de340a23",
   "metadata": {},
   "source": [
    "#### Embedding model - OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c012600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "#   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b11d1a",
   "metadata": {},
   "source": [
    "#### Load PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294bd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    './data/houndofbaskervil00doyluoft.pdf'\n",
    ")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = [] #List of Document\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb39c8",
   "metadata": {},
   "source": [
    "#### Test that the document is loaded properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata = \n",
      " {'producer': 'Internet Archive PDF 1.4.22; including mupdf and pymupdf/skimage', 'creator': 'Internet Archive', 'creationdate': '2023-10-24T08:24:30+00:00', 'title': 'The hound of the Baskervilles : another adventure of Sherlock Holmes', 'keywords': 'https://archive.org/details/houndofbaskervil00doyluoft', 'author': 'Doyle, Arthur Conan, Sir, 1859-1930; Paget, Sidney, 1861-1908', 'moddate': '2023-10-24T08:24:30+00:00', 'source': './data/houndofbaskervil00doyluoft.pdf', 'total_pages': 408, 'page': 0, 'page_label': ''}\n",
      "Content = \n",
      " The  Hound  of  the  Baskervilles \n",
      "from  his  friends  of  the  C.C.H.,\"  was  en- \n",
      "graved upon  it,  with  the  date  \"  1884.\"  It \n",
      "was  just  such  a  stick  as  the  old-fashioned \n",
      "family  practitioner  used  to  carry â€” dignified, \n",
      "solid,  and  reassuring. \n",
      "\"  Well,  Watson,  what  do  you  make  of  it  ?  \" \n",
      "Holmes  was  sitting  with  his  back  to  me, \n",
      "and  I  had  given  him  no  sign  \n",
      "\n",
      " Total characters = 1097\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metadata = \\n {pages[0].metadata}\")\n",
    "\n",
    "# Print first 500 characters of 15th page\n",
    "print(f\"Content = \\n {pages[25].page_content[:100]}\")\n",
    "\n",
    "# Total characters in 15th page\n",
    "print(f\"\\n Total characters = {len(pages[25].page_content)}\")\n",
    "\n",
    "# Print the 35th page content, but not evertything, only first 300 characters\n",
    "print(f\"\\n Content = \\n {pages[35].page_content[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9548a90",
   "metadata": {},
   "source": [
    "#### Split the document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "990877eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks = 696\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Number of chunks = {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8e164",
   "metadata": {},
   "source": [
    "#### Index chunks of the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19dbe666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=\"./chroma_langchain_db\",\n",
    "    collection_name=\"rag_demo\",\n",
    "    #embedding_model=sbert_model,\n",
    ")\n",
    "\n",
    "_ = vector_store.add_documents(documents=chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370b93f",
   "metadata": {},
   "source": [
    "#### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c00160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ea002",
   "metadata": {},
   "source": [
    "#### Build & Compile Graph for Retrieval & Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e78d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Retrieve similar docs from vector db\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], k=2)\n",
    "    return {\"context\": retrieved_docs, \"question\": state[\"question\"]}\n",
    "\n",
    "# Lambda function to generate response\n",
    "def generate_response(prompt_text: str) -> str:\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        # max_new_tokens=200, \n",
    "        max_length=1000,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate answer using LLM\n",
    "def generate_answer(state: State):\n",
    "    docs_context = \"\\n\".join([doc.page_content for doc in state[\"context\"]])\n",
    "    messages = prompt.invoke(\n",
    "        {\n",
    "            \"context\": docs_context,\n",
    "            \"question\": state[\"question\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #print(f\"Prompt = \\n {messages}\")\n",
    "\n",
    "    prompt_text = messages.to_string()\n",
    "\n",
    "    # Generate response using GPT-2\n",
    "    response_text = generate_response(prompt_text)\n",
    "\n",
    "    \n",
    "    # Generate response using GPT-2\n",
    "    response_text = generate_response(prompt_text)\n",
    "    return {\"answer\": response_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c21fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: Who stole the shoe? \n",
      "Context: The  Hound  of  the  Baskervilles \n",
      "sucker  in    this  hotel,\"  he   cried.     \"  They'll \n",
      "find  they've  started  in  to  monkey  with  the \n",
      "wrong   man    unless    they  are   careful.     By \n",
      "thunder,  if  that  chap  can't  find  my  missing \n",
      "boot  there  will  be  trouble.    I  can  take  a  joke \n",
      "with  the  best,  Mr.  Holmes,  but  they've  got \n",
      "a  bit  over  the  mark  this  time.\" \n",
      "\"  Still  looking  for  your  boot  ?'* \n",
      "\"  Yes,  sir,  and  mean  to  find  it.\" \n",
      "\"  But,  surely,  you  said  that  it  was  a  new \n",
      "brown  boot  ?  \" \n",
      "\"So  it  was,  sir.  And  now  it's  an  old \n",
      "black  one.\" \n",
      "\"  What !  you  don't  mean  to  say   ?  \" \n",
      "**  That's  just  what  I  do  mean  to  say.  I \n",
      "only  had  three  pairs  in  the  world â€” the  new \n",
      "brown,  the  old  black,  and  the  patent  leathers, \n",
      "which  I  am  wearing.  Last  night  they  took \n",
      "one  of  my  brown  ones,  and  to-day  they  have \n",
      "sneaked  one  of  the  black.  Well,  have  you\n",
      "The  Hound  of  the  Baskervilles \n",
      "sucker  in    this  hotel,\"  he   cried.     \"  They'll \n",
      "find  they've  started  in  to  monkey  with  the \n",
      "wrong   man    unless    they  are   careful.     By \n",
      "thunder,  if  that  chap  can't  find  my  missing \n",
      "boot  there  will  be  trouble.    I  can  take  a  joke \n",
      "with  the  best,  Mr.  Holmes,  but  they've  got \n",
      "a  bit  over  the  mark  this  time.\" \n",
      "\"  Still  looking  for  your  boot  ?'* \n",
      "\"  Yes,  sir,  and  mean  to  find  it.\" \n",
      "\"  But,  surely,  you  said  that  it  was  a  new \n",
      "brown  boot  ?  \" \n",
      "\"So  it  was,  sir.  And  now  it's  an  old \n",
      "black  one.\" \n",
      "\"  What !  you  don't  mean  to  say   ?  \" \n",
      "**  That's  just  what  I  do  mean  to  say.  I \n",
      "only  had  three  pairs  in  the  world â€” the  new \n",
      "brown,  the  old  black,  and  the  patent  leathers, \n",
      "which  I  am  wearing.  Last  night  they  took \n",
      "one  of  my  brown  ones,  and  to-day  they  have \n",
      "sneaked  one  of  the  black.  Well,  have  you \n",
      "Answer:  \" I  only  had  three  pairs  in  the  world â€” the  new \n",
      "brown,  the  old  black,  and  the  patent  leathers, \n",
      "which  I  am  wearing.  Last  night  they  took  one  of  my  brown  ones,  and  to-day  they  have \n",
      "sneaked  one  of  the  black.  Well,  have  you  Answer:  \" I  only had  three  pairs  in\n"
     ]
    }
   ],
   "source": [
    "question = \"Who stole the shoe?\"\n",
    "state_input = {\"question\": question}\n",
    "result = generate_answer(retrieve(state_input))\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a5886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
